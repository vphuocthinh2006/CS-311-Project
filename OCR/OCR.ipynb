import re
import pymupdf
import pytesseract
from pdf2image import convert_from_path
import json

def clean_extracted_text(text):
    """
    Clean and format extracted text by removing page markers and fixing spacing
    """
    if not text:
        return ""
    
    # Remove page markers and common PDF artifacts
    page_patterns = [
        r'(?i)page\s+\d+(?:\s+of\s+\d+)?',  # "Page 1", "Page 1 of 5"
        r'(?i)página\s+\d+',                 # Spanish page markers
        r'^\s*\d+\s*$',                      # Standalone page numbers on their own line
        r'(?i)page\s*\|\s*\d+',             # "Page | 1"
        r'(?i)\d+\s*/\s*\d+',               # "1/5", "2/5"
        r'(?i)\d+\s+of\s+\d+',              # "1 of 5"
    ]
    
    # Split into lines for processing
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        original_line = line
        line = line.strip()
        
        # Skip empty lines
        if not line:
            continue
            
        # Check if line matches any page pattern
        is_page_marker = False
        for pattern in page_patterns:
            if re.match(pattern, line):
                is_page_marker = True
                break
        
        if not is_page_marker:
            # Clean up spacing within the line
            cleaned_line = re.sub(r'\s+', ' ', line)
            cleaned_lines.append(cleaned_line)
    
    # Join lines back together with proper spacing
    result = '\n'.join(cleaned_lines)
    
    # Remove excessive blank lines (more than 2 consecutive)
    result = re.sub(r'\n{3,}', '\n\n', result)
    
    return result.strip()

def extract_text_hybrid_fixed(pdf_path, dpi=300, lang="eng", min_char=50):
    """
    Fixed version of hybrid text extraction with proper error handling
    """
    try:
        doc = pymupdf.open(pdf_path)
        text_output = ""
        
        # Convert PDF to images for OCR fallback
        print("Converting PDF to images...")
        images = convert_from_path(pdf_path, dpi=dpi)
        print(f"Converted {len(images)} pages")
        
        for page_num, page in enumerate(doc):
            print(f"Processing page {page_num + 1}...")
            page_text = ""
            
            # Try digital text extraction first
            blocks = page.get_text("blocks")
            
            if blocks:
                # Sort blocks by position (top to bottom, left to right)
                blocks = sorted(blocks, key=lambda b: (b[1], b[0]))
                
                # Extract text from blocks
                block_texts = []
                for block in blocks:
                    block_content = block[4].strip()
                    if block_content:
                        block_texts.append(block_content)
                
                # Join blocks with newlines to preserve structure
                page_text = '\n'.join(block_texts)
            
            # Check if we got enough meaningful text
            if len(page_text.strip()) >= min_char:
                # Use digital extraction
                text_output += page_text + "\n\n"
            else:
                # Fall back to OCR
                print(f"  Using OCR for page {page_num + 1}")
                if page_num < len(images):
                    ocr_text = pytesseract.image_to_string(images[page_num], lang=lang)
                    if ocr_text.strip():
                        text_output += ocr_text + "\n\n"
        
        doc.close()
        
        # Clean the final text
        cleaned_text = clean_extracted_text(text_output)
        
        return cleaned_text
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        import traceback
        traceback.print_exc()
        return ""

def categorize_resume_text(text):
    """
    Categorize extracted text into resume sections
    """
    if not text:
        return {"error": "No text to categorize"}
    
    # Define section keywords
    section_keywords = {
        'contact_info': ['email', 'phone', 'tel', 'mobile', 'address', 'linkedin', 'github', 'portfolio', 'contact'],
        'summary': ['summary', 'objective', 'profile', 'about', 'overview', 'introduction', 'career objective'],
        'experience': ['experience', 'employment', 'work history', 'professional experience', 'career history', 'work experience'],
        'education': ['education', 'academic', 'degree', 'university', 'college', 'school', 'qualification'],
        'skills': ['skills', 'technical skills', 'competencies', 'technologies', 'tools', 'programming', 'frameworks'],
        'projects': ['projects', 'portfolio', 'accomplishments', 'achievements', 'personal projects'],
        'certifications': ['certifications', 'certificates', 'awards', 'licenses', 'credentials']
    }
    
    # Split text into lines
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    
    # Initialize sections
    sections = {
        'contact_info': [],
        'summary': [],
        'experience': [],
        'education': [],
        'skills': [],
        'projects': [],
        'certifications': [],
        'uncategorized': []
    }
    
    current_section = 'uncategorized'
    
    for line in lines:
        # Check if this line is a section header
        line_lower = line.lower()
        section_found = None
        
        for section, keywords in section_keywords.items():
            for keyword in keywords:
                if keyword in line_lower and len(line) < 100:
                    # Additional check: line should be relatively short and likely a header
                    if line_lower.strip() == keyword or line_lower.strip().startswith(keyword):
                        section_found = section
                        break
            if section_found:
                break
        
        if section_found:
            current_section = section_found
        
        # Add line to current section
        sections[current_section].append(line)
    
    # Extract contact information using regex patterns
    contact_patterns = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'[\+]?[0-9]?[\-\s\.]?\(?[0-9]{3,4}\)?[\-\s\.]?[0-9]{3,4}[\-\s\.]?[0-9]{3,6}',
        'linkedin': r'linkedin\.com/in/[\w-]+',
        'github': r'github\.com/[\w-]+'
    }
    
    extracted_contacts = {}
    for contact_type, pattern in contact_patterns.items():
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            extracted_contacts[contact_type] = list(set(matches))  # Remove duplicates
    
    return {
        'categorized_sections': sections,
        'extracted_contacts': extracted_contacts,
        'statistics': {
            'total_lines': len(lines),
            'total_characters': len(text),
            'sections_with_content': sum(1 for section, content in sections.items() 
                                       if content and section != 'uncategorized')
        }
    }

def print_resume_results(text, categorized_data):
    """
    Print formatted results
    """
    print("=" * 60)
    print("CLEANED TEXT EXTRACTION")
    print("=" * 60)
    print(text[:1500] + "..." if len(text) > 1500 else text)
    print("\n")
    
    if 'error' not in categorized_data:
        print("=" * 60)
        print("CATEGORIZATION RESULTS")
        print("=" * 60)
        
        stats = categorized_data['statistics']
        print(f"Total lines: {stats['total_lines']}")
        print(f"Total characters: {stats['total_characters']}")
        print(f"Sections found: {stats['sections_with_content']}")
        print()
        
        # Show extracted contacts
        if categorized_data['extracted_contacts']:
            print("EXTRACTED CONTACTS:")
            for contact_type, values in categorized_data['extracted_contacts'].items():
                for value in values:
                    print(f"  {contact_type.upper()}: {value}")
            print()
        
        # Show categorized sections
        for section, content in categorized_data['categorized_sections'].items():
            if content:
                print(f"{section.replace('_', ' ').upper()}:")
                for i, item in enumerate(content):
                    if i < 5:  # Show first 5 items
                        print(f"  • {item}")
                    elif i == 5:
                        print(f"  ... and {len(content) - 5} more items")
                        break
                print()

if __name__ == "__main__":
    pdf_path = "PATH"
    print("Starting PDF text extraction...")
    # Extract clean text
    extracted_text = extract_text_hybrid_fixed(pdf_path, dpi=300, lang="eng", min_char=50)
    if extracted_text:
        # Categorize the text
        categorized_data = categorize_resume_text(extracted_text)
        
        # Print results
        print_resume_results(extracted_text, categorized_data)
        
        # Save to JSON file
        result_data = {
            'raw_text': extracted_text,
            **categorized_data
        }
        
        with open('resume_extraction_results.json', 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        print("Results saved to 'resume_extraction_results.json'")
    else:
        print("Failed to extract text from PDF")

def extract_text_hybrid(pdf_path, dpi=300, lang="eng", min_char=20):
    """
    Direct replacement for your original function - simplified and fixed
    """
    return extract_text_hybrid_fixed(pdf_path, dpi, lang, min_char)
