{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13267195,"sourceType":"datasetVersion","datasetId":8407443},{"sourceId":13343097,"sourceType":"datasetVersion","datasetId":8461307},{"sourceId":13346624,"sourceType":"datasetVersion","datasetId":8463954}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\n\n# Đường dẫn tới thư mục chứa các file JSON\nfolder_path = \"/kaggle/input/data001/ResumesJsonAnnotated\"\ndata_ls = []\n# Duyệt qua toàn bộ file trong thư mục\nfor filename in os.listdir(folder_path):\n    if filename.endswith(\".json\"):\n        file_path = os.path.join(folder_path, filename)\n        \n        # Đọc nội dung file JSON\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n            data_ls.append(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:22:49.747658Z","iopub.execute_input":"2025-10-12T08:22:49.748180Z","iopub.status.idle":"2025-10-12T08:22:53.188911Z","shell.execute_reply.started":"2025-10-12T08:22:49.748127Z","shell.execute_reply":"2025-10-12T08:22:53.188385Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_and_align_labels(text, annotations):\n    # tạo mảng nhãn với độ dài bằng số ký tự\n    labels = [\"O\"] * len(text)\n    # gán nhãn cho từng ký tự\n    for start, end, label in annotations:\n        tag = label.split(\":\")[0]  \n        labels[start] = f\"B-{tag}\" # kí tự bắt đầu là B-SKILL\n        for i in range(start + 1, end):\n            labels[i] = f\"I-{tag}\" # kí tự thuộc skill nhưng không phải kí tự bắt đầu sẽ là I-SKILL\n    \n    # tokenization\n    tokenized_inputs = tokenizer(\n        text,\n        truncation=True,\n        return_offsets_mapping=True, # Trả về offset mapping (index cho kí tự bắt đầu và kết thúc của 1 token)\n        padding=\"max_length\",\n        max_length=512\n    )\n\n    # ánh xạ từ char-level sang token-level\n    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n    label_ids = []\n    for offsets in offset_mapping:\n        if offsets[0] == offsets[1]:\n            label_ids.append(-100) # các token đặc biệt như [CLS] hay [SEP] sẽ đc gán là -100 \n        else:\n            label_ids.append(\n                0 if all(l == \"O\" for l in labels[offsets[0]:offsets[1]]) # nếu ko có SKILL → 0\n                else 1  # nếu có SKILL → 1\n            )\n\n    tokenized_inputs[\"labels\"] = label_ids\n    return tokenized_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:27:21.339721Z","iopub.execute_input":"2025-10-12T08:27:21.340284Z","iopub.status.idle":"2025-10-12T08:27:22.081357Z","shell.execute_reply.started":"2025-10-12T08:27:21.340259Z","shell.execute_reply":"2025-10-12T08:27:22.080694Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40babda816448c79219b60ae4e52968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87d65eb9f6146cf94b0621fdd627eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f3a1ddf093343a5927bbdc29be968bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61dcd7531aa64b4db770a9946af8a9dd"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    # Loại bỏ surrogate characters (những ký tự Unicode lỗi)\n    return re.sub(r'[\\ud800-\\udfff]', '', text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:43:23.778382Z","iopub.execute_input":"2025-10-12T08:43:23.779105Z","iopub.status.idle":"2025-10-12T08:43:23.782975Z","shell.execute_reply.started":"2025-10-12T08:43:23.779077Z","shell.execute_reply":"2025-10-12T08:43:23.782182Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"ner_inputs = []\nfor example in data_ls:\n    text = clean_text(example[\"text\"])\n    ner_inputs.append(tokenize_and_align_labels(text,example['annotations']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:43:52.836156Z","iopub.execute_input":"2025-10-12T08:43:52.836406Z","iopub.status.idle":"2025-10-12T08:44:07.577274Z","shell.execute_reply.started":"2025-10-12T08:43:52.836390Z","shell.execute_reply":"2025-10-12T08:44:07.576678Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from datasets import Dataset\n\n# chuyển sang format dataset của huggingface\ndataset = Dataset.from_list(ner_inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:50:08.349649Z","iopub.execute_input":"2025-10-12T08:50:08.350369Z","iopub.status.idle":"2025-10-12T08:50:10.026752Z","shell.execute_reply.started":"2025-10-12T08:50:08.350344Z","shell.execute_reply":"2025-10-12T08:50:10.026179Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import os\n\n# vô hiệu hóa WANDB để trainer hoạt động được trên kaggle\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:55:54.815904Z","iopub.execute_input":"2025-10-12T08:55:54.816223Z","iopub.status.idle":"2025-10-12T08:55:54.820488Z","shell.execute_reply.started":"2025-10-12T08:55:54.816198Z","shell.execute_reply":"2025-10-12T08:55:54.819683Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\n# dict ánh xạ qua lại giữa nhãn và chỉ số của nó\nlabel2id = {\"O\": 0, \"B-SKILL\": 1, \"I-SKILL\": 2}\nid2label = {v: k for k, v in label2id.items()}\n\n# khởi tạo model với tác vụ TokenClassification (NER), sử dụng BERT\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\nargs = TrainingArguments(\n    output_dir=\"./bert-ner-skill\", # đường dẫn lưu checkpoint\n    eval_strategy=\"no\", # không evaluate model lúc train \n    learning_rate=1e-5,\n    per_device_train_batch_size=8, # mỗi GPU train 8 mẫu trong 1 bước huấn luyện\n    num_train_epochs=10,\n    weight_decay=0.01, # giảm các trọng số có giá trị quá lớn, giúp model bớt overfitting\n    logging_steps=20, # in loss mỗi 20 step\n    save_strategy=\"epoch\" # lưu model mỗi epoch\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    tokenizer=tokenizer\n)\n\n# huấn luyện model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:55:57.859384Z","iopub.execute_input":"2025-10-12T08:55:57.859912Z","iopub.status.idle":"2025-10-12T09:47:04.329410Z","shell.execute_reply.started":"2025-10-12T08:55:57.859888Z","shell.execute_reply":"2025-10-12T09:47:04.328811Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/650850805.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3150' max='3150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3150/3150 51:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.591000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.319100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.270600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.254200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.208900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.188800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.190200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.170100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.168000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.158000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.152100</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.151000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.142000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.131600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.133800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.126100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.120400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.116600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.110900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.111200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.102200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.105000</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.102100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.102100</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.099300</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.096300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.104500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.095900</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.092300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.087500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.089000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.094000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.086900</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.084600</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.080700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.079500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.077800</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.076800</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.077000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.081900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.075200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.070100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.069400</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.070000</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.069800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.068800</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.069700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.072900</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.069600</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.072700</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.066800</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.066500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.066800</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.067000</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.072600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.068200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.061600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.059300</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.064300</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.061200</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.061900</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.061000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.059900</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.063000</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.064600</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.061900</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.060100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.062800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.058900</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.062100</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.061800</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.052600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.056500</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.056400</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.058100</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.057100</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.053800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.056000</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.059000</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.055500</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.056200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.053400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.053200</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.057300</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.059500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.057700</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.051900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.051800</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.052500</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.046400</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.052900</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.053300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.052600</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.053900</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.051200</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.054200</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.053200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.055100</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.050500</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.048400</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.051800</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.051600</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.050800</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.048800</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.054700</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.047100</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.049400</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.046900</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.052200</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.046400</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.048500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.045000</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.049300</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.047000</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.048000</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.054600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.049300</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.046600</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.046400</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.045700</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.047500</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.047000</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.050600</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.046100</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.046800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.045200</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.044600</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.046500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.048700</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.046300</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.047000</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.044700</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.048800</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.045700</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.044300</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.046900</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.046100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.042500</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.044800</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.047400</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.048700</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.046300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.045300</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.045700</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.045600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3150, training_loss=0.07892444163087814, metrics={'train_runtime': 3065.5181, 'train_samples_per_second': 16.405, 'train_steps_per_second': 1.028, 'total_flos': 1.314073269974016e+16, 'train_loss': 0.07892444163087814, 'epoch': 10.0})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\nmodel_dir = \"/kaggle/working/bert-ner-skill/checkpoint-3150\"  # đường dẫn đã lưu model\n\n# load model đã train\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForTokenClassification.from_pretrained(model_dir)\n# xây dựng pipeline, không cần preprocess text để đưa vào model\nner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T09:49:53.842601Z","iopub.execute_input":"2025-10-12T09:49:53.843233Z","iopub.status.idle":"2025-10-12T09:49:54.102223Z","shell.execute_reply.started":"2025-10-12T09:49:53.843207Z","shell.execute_reply":"2025-10-12T09:49:54.101299Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"text = \"\"\"One97 Communications Limited Data Scientist Jan 2019 to Till Date Detect important information from images and redact required fields. YOLO CNN Object-detection, OCR Insights, find anomaly or performance drop in all possible sub-space. Predict the Insurance claim probability. Estimate the premium amount to be charged B.Tech(Computer Science) from SGBAU university in 2017. M.Tech (Computer Science Engineering) from Indian Institute of Technology (IIT), Kanpur in 2019WORK EXPERIENCE EDUCATIONMACY WILLIAMS DATA SCIENTIST Data Scientist working on problems related to market research and customer analysis. I want to expand my arsenal of application building and work on different kinds of problems. Looking for a role where I can work with a coordinative team and exchange knowledge during the process. Java, C++, Python, Machine Learning, Algorithms, Natural Language Processing, Deep Learning, Computer Vision, Pattern Recognition, Data Science, Data Analysis, Software Engineer, Data Analyst, C, PySpark, Kubeflow.ABOUT SKILLS Customer browsing patterns. Predict potential RTO(Return To Origin) orders for e- commerce. Object Detection.PROJECTS ACTIVITES\"\"\"\nresults = ner_pipeline(text)\n\nfor r in results:\n    print(f\"{r['word']} → {r['entity_group']} (score={r['score']:.2f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:02:49.755084Z","iopub.execute_input":"2025-10-12T10:02:49.755678Z","iopub.status.idle":"2025-10-12T10:02:49.801524Z","shell.execute_reply.started":"2025-10-12T10:02:49.755657Z","shell.execute_reply":"2025-10-12T10:02:49.800876Z"}},"outputs":[{"name":"stdout","text":"communications → SKILL (score=1.00)\ndata → SKILL (score=1.00)\nscientist → SKILL (score=1.00)\ninformation → SKILL (score=1.00)\nimages → SKILL (score=1.00)\nfields → SKILL (score=1.00)\ncnn → SKILL (score=0.99)\ndetection → SKILL (score=1.00)\no → SKILL (score=1.00)\n##cr → SKILL (score=1.00)\ninsights → SKILL (score=1.00)\nperformance → SKILL (score=1.00)\nspace → SKILL (score=1.00)\ninsurance → SKILL (score=1.00)\nprobability → SKILL (score=0.95)\npremium → SKILL (score=1.00)\ntech → SKILL (score=1.00)\ncomputer → SKILL (score=0.89)\nscience → SKILL (score=0.90)\nm → SKILL (score=0.97)\nscience → SKILL (score=0.58)\nengineering → SKILL (score=1.00)\ntechnology → SKILL (score=1.00)\nproblems → SKILL (score=1.00)\nmarket → SKILL (score=0.98)\nresearch → SKILL (score=0.99)\ncustomer → SKILL (score=1.00)\nanalysis → SKILL (score=1.00)\napplication → SKILL (score=1.00)\nbuilding → SKILL (score=1.00)\nwork → SKILL (score=1.00)\ncan → SKILL (score=1.00)\nteam → SKILL (score=1.00)\nexchange → SKILL (score=1.00)\nknowledge → SKILL (score=1.00)\nprocess → SKILL (score=1.00)\njava → SKILL (score=1.00)\nc → SKILL (score=0.66)\npython → SKILL (score=1.00)\nmachine → SKILL (score=0.90)\nlearning → SKILL (score=0.98)\nalgorithms → SKILL (score=1.00)\nnatural → SKILL (score=0.96)\nlanguage → SKILL (score=1.00)\nprocessing → SKILL (score=0.99)\ndeep → SKILL (score=0.87)\nlearning → SKILL (score=0.91)\ncomputer → SKILL (score=0.92)\nvision → SKILL (score=1.00)\npattern → SKILL (score=0.98)\nrecognition → SKILL (score=1.00)\ndata → SKILL (score=0.57)\nsoftware → SKILL (score=1.00)\nengineer → SKILL (score=1.00)\ndata → SKILL (score=0.87)\nanalyst → SKILL (score=0.99)\np → SKILL (score=0.98)\n##ys → SKILL (score=0.99)\n##park → SKILL (score=0.98)\nku → SKILL (score=0.96)\n##be → SKILL (score=0.89)\n##flow → SKILL (score=0.92)\nskills → SKILL (score=1.00)\npatterns → SKILL (score=0.96)\nrt → SKILL (score=1.00)\n##o → SKILL (score=1.00)\ncommerce → SKILL (score=1.00)\nprojects → SKILL (score=1.00)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# hàm gộp các skill (A, ##B -> AB) và trả về danh sánh skill\ndef get_output(text):\n    results = ner_pipeline(text)\n    \n    skill_ls = []\n    leng = 0\n    prev_skill = \"\"\n    for r in results:\n        cur_skill = r['word']\n        if cur_skill[0] == '#':\n            cur_skill = cur_skill[2:]\n            skill = prev_skill + cur_skill\n            skill_ls[len(skill_ls)-1]=skill\n        else:\n            skill_ls.append(cur_skill)\n        prev_skill = cur_skill\n    return skill_ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:11:13.090183Z","iopub.execute_input":"2025-10-12T10:11:13.090453Z","iopub.status.idle":"2025-10-12T10:11:13.095225Z","shell.execute_reply.started":"2025-10-12T10:11:13.090433Z","shell.execute_reply":"2025-10-12T10:11:13.094450Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"text = \"\"\"One97 Communications Limited Data Scientist Jan 2019 to Till Date Detect important information from images and redact required fields. YOLO CNN Object-detection, OCR Insights, find anomaly or performance drop in all possible sub-space. Predict the Insurance claim probability. Estimate the premium amount to be charged B.Tech(Computer Science) from SGBAU university in 2017. M.Tech (Computer Science Engineering) from Indian Institute of Technology (IIT), Kanpur in 2019WORK EXPERIENCE EDUCATIONMACY WILLIAMS DATA SCIENTIST Data Scientist working on problems related to market research and customer analysis. I want to expand my arsenal of application building and work on different kinds of problems. Looking for a role where I can work with a coordinative team and exchange knowledge during the process. Java, C++, Python, Machine Learning, Algorithms, Natural Language Processing, Deep Learning, Computer Vision, Pattern Recognition, Data Science, Data Analysis, Software Engineer, Data Analyst, C, PySpark, Kubeflow.ABOUT SKILLS Customer browsing patterns. Predict potential RTO(Return To Origin) orders for e- commerce. Object Detection.PROJECTS ACTIVITES\"\"\"\nskill_ls = get_output(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:11:37.447987Z","iopub.execute_input":"2025-10-12T10:11:37.448287Z","iopub.status.idle":"2025-10-12T10:11:37.491755Z","shell.execute_reply.started":"2025-10-12T10:11:37.448265Z","shell.execute_reply":"2025-10-12T10:11:37.491184Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# hàm so sánh giữa candidate và company\ndef comparision(cd,cp):\n    cd_ls = get_output(cd)\n    cp_ls = get_output(cp)\n\n    match = []\n    not_match = []\n\n    for rq in cp_ls:\n        matched = False\n        for skill in cd_ls:\n            if(rq == skill):\n                match.append(rq)\n                matched = True\n                break\n        if(matched == False):\n            not_match.append(rq)\n    return match, not_match","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:23:19.071000Z","iopub.execute_input":"2025-10-12T10:23:19.071781Z","iopub.status.idle":"2025-10-12T10:23:19.076487Z","shell.execute_reply.started":"2025-10-12T10:23:19.071755Z","shell.execute_reply":"2025-10-12T10:23:19.075706Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"cp = \"\"\"User of Bitwarden\nStartup experience\nOpen source experience\nExperience in SQL Server, Azure, Node.js, Electron, RabbitMQ, Angular, .NET Core, web browser extensions\"\"\"\n\ncd = \"\"\" Motivated to build a career in development.\nProactive attitude with curiosity to experiment.\nEagerness to learn modern frameworks such as React, Angular, or Django.\nEffective communication skills in English.\nFamiliarity with GitHub or version control is a plus.\nComfortable with self-paced online learning\"\"\"\n\nmatch, not_match = comparision(cd,cp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:23:23.612937Z","iopub.execute_input":"2025-10-12T10:23:23.613307Z","iopub.status.idle":"2025-10-12T10:23:23.648770Z","shell.execute_reply.started":"2025-10-12T10:23:23.613284Z","shell.execute_reply":"2025-10-12T10:23:23.648271Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"print(match)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:23:25.774734Z","iopub.execute_input":"2025-10-12T10:23:25.775273Z","iopub.status.idle":"2025-10-12T10:23:25.779036Z","shell.execute_reply.started":"2025-10-12T10:23:25.775251Z","shell.execute_reply":"2025-10-12T10:23:25.778309Z"}},"outputs":[{"name":"stdout","text":"['angular']\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"print(not_match)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:23:28.942124Z","iopub.execute_input":"2025-10-12T10:23:28.942430Z","iopub.status.idle":"2025-10-12T10:23:28.946544Z","shell.execute_reply.started":"2025-10-12T10:23:28.942407Z","shell.execute_reply":"2025-10-12T10:23:28.945858Z"}},"outputs":[{"name":"stdout","text":"['startup', 'experience', 'open', 'source', 'experience', 'sql', 'server', 'azure', 'node', '.', 'js', 'electron', 'mq', '.', 'net', 'core', 'web', 'browser', 'extensions']\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"bert-base-uncased-10-epochs\", \"zip\", \"/kaggle/working/bert-ner-skill/checkpoint-3150\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:31:07.109701Z","iopub.execute_input":"2025-10-12T10:31:07.109943Z","iopub.status.idle":"2025-10-12T10:32:06.737325Z","shell.execute_reply.started":"2025-10-12T10:31:07.109928Z","shell.execute_reply":"2025-10-12T10:32:06.736670Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/bert-base-uncased-10-epochs.zip'"},"metadata":{}}],"execution_count":58}]}